import { TranscriptSegment } from '../types'
import OpenAI from 'openai'
import { GoogleGenerativeAI } from '@google/generative-ai'
import * as fs from 'fs'
import FormData from 'form-data'

export interface TranscriptionResult {
  success: boolean
  segments: TranscriptSegment[]
  error?: string
  processingTime?: number
}

export class TranscriptionService {
  private static openaiApiKey: string | undefined
  private static openai: OpenAI | undefined

  /**
   * Inicializa o servi√ßo de transcri√ß√£o
   */
  static initialize(): void {
    this.openaiApiKey = process.env.OPENAI_API_KEY
    
    if (this.openaiApiKey) {
      this.openai = new OpenAI({
        apiKey: this.openaiApiKey,
        timeout: 120000, // 120s
        maxRetries: 2,
      })
      console.log(`ü§ñ Servi√ßo de transcri√ß√£o inicializado com OpenAI`)
    } else {
      console.log(`‚ö†Ô∏è OpenAI API Key n√£o configurada`)
    }
  }

  /**
   * Transcreve √°udio via OpenAI Whisper (REAL) com retry
   */
  static async transcribeWithOpenAI(audioFilePath: string, maxRetries: number = 3): Promise<TranscriptionResult> {
    for (let attempt = 1; attempt <= maxRetries; attempt++) {
      try {
        if (!this.openai || !this.openaiApiKey) {
          return {
            success: false,
            segments: [],
            error: 'OpenAI API Key n√£o configurada'
          }
        }

        if (!fs.existsSync(audioFilePath)) {
          return {
            success: false,
            segments: [],
            error: `Arquivo de √°udio n√£o encontrado: ${audioFilePath}`
          }
        }

        console.log(`üéµ Transcrevendo √°udio REAL via OpenAI Whisper (tentativa ${attempt}/${maxRetries}): ${audioFilePath}`)
        const startTime = Date.now()

        // Verificar tamanho do arquivo
        const stats = fs.statSync(audioFilePath)
        const fileSizeInMB = stats.size / (1024 * 1024)
        console.log(`üìä Tamanho do arquivo: ${fileSizeInMB.toFixed(2)} MB`)

        // OpenAI Whisper tem limite de 25MB
        if (fileSizeInMB > 25) {
          console.log(`‚ö†Ô∏è Arquivo muito grande (${fileSizeInMB.toFixed(2)} MB), OpenAI Whisper suporta at√© 25MB`)
          return {
            success: false,
            segments: [],
            error: `Arquivo muito grande: ${fileSizeInMB.toFixed(2)} MB. OpenAI Whisper suporta at√© 25MB`
          }
        }

        // Fazer transcri√ß√£o REAL via OpenAI Whisper (SDK)
        const transcription = await this.openai.audio.transcriptions.create({
          file: fs.createReadStream(audioFilePath),
          model: "whisper-1",
          language: "pt", // Portugu√™s
          response_format: "verbose_json",
          timestamp_granularities: ["segment"]
        })

        const processingTime = Date.now() - startTime
        console.log(`‚úÖ Transcri√ß√£o OpenAI REAL conclu√≠da em ${processingTime}ms (tentativa ${attempt})`)

        // Converter resultado para nosso formato
        const segments = this.convertOpenAIResultToSegments(transcription)
        
        return {
          success: true,
          segments,
          processingTime
        }

      } catch (error) {
        console.error(`‚ùå Erro na transcri√ß√£o OpenAI REAL (tentativa ${attempt}/${maxRetries}):`, error)
        
        // √öltima tentativa: tentar via HTTP direto (multipart) como fallback
        if (attempt === maxRetries) {
          try {
            console.log('üåê Tentando fallback via HTTP direto (multipart)')
            const result = await this.transcribeViaHttp(audioFilePath)
            if (result.success) return result
            return result
          } catch (httpErr) {
            return {
              success: false,
              segments: [],
              error: `Falha ap√≥s ${maxRetries} tentativas e fallback HTTP: ${(httpErr as Error).message}`
            }
          }
        }
        
        // Aguardar antes da pr√≥xima tentativa (backoff exponencial)
        const waitTime = Math.min(1000 * Math.pow(2, attempt - 1), 10000) // 1s, 2s, 4s, 8s, 10s max
        console.log(`‚è≥ Aguardando ${waitTime}ms antes da pr√≥xima tentativa...`)
        await new Promise(resolve => setTimeout(resolve, waitTime))
      }
    }
    
    return {
      success: false,
      segments: [],
      error: 'N√∫mero m√°ximo de tentativas excedido'
    }
  }

  /**
   * Fallback: chamada HTTP direta ao endpoint de transcri√ß√£o
   */
  private static async transcribeViaHttp(audioFilePath: string): Promise<TranscriptionResult> {
    if (!this.openaiApiKey) {
      return { success: false, segments: [], error: 'OpenAI API Key n√£o configurada' }
    }
    if (!fs.existsSync(audioFilePath)) {
      return { success: false, segments: [], error: `Arquivo n√£o encontrado: ${audioFilePath}` }
    }

    const startTime = Date.now()
    
    try {
      // Ler arquivo como Buffer para evitar problemas com FormData
      const audioBuffer = fs.readFileSync(audioFilePath)
      console.log(`üìä Lendo arquivo como Buffer: ${audioBuffer.length} bytes`)
      
      const form = new FormData()
      // Campos exigidos
      form.append('model', 'whisper-1')
      form.append('language', 'pt')
      form.append('response_format', 'verbose_json')
      // Campo array deve usar [] em multipart
      form.append('timestamp_granularities[]', 'segment')
      
      // Criar Blob a partir do Buffer
      const audioBlob = new Blob([audioBuffer], { type: 'audio/mpeg' })
      form.append('file', audioBlob, 'audio.mp3')

      console.log(`üåê Enviando requisi√ß√£o HTTP direta para OpenAI...`)
      
             const res = await fetch('https://api.openai.com/v1/audio/transcriptions', {
         method: 'POST',
         headers: {
           Authorization: `Bearer ${this.openaiApiKey}`,
         },
         body: form as any,
       })

      if (!res.ok) {
        const errTxt = await res.text().catch(() => '')
        console.log(`‚ùå HTTP ${res.status}: ${errTxt}`)
        return {
          success: false,
          segments: [],
          error: `HTTP ${res.status} - ${errTxt}`
        }
      }

      const json: any = await res.json()
      console.log(`‚úÖ Fallback HTTP funcionou!`)
      const segments = this.convertOpenAIResultToSegments(json)
      const processingTime = Date.now() - startTime
      return { success: true, segments, processingTime }
      
    } catch (error) {
      console.error('‚ùå Erro no fallback HTTP:', error)
      return {
        success: false,
        segments: [],
        error: `Fallback HTTP falhou: ${(error as Error).message}`
            }
    }
  }

  /**
   * Converte resultado do Gemini para nosso formato
   */
  private static convertGeminiResultToSegments(transcriptionText: string): TranscriptSegment[] {
    try {
      console.log(`üîÑ Convertendo resultado do Gemini...`)
      
      const segments: TranscriptSegment[] = []
      const words = transcriptionText.split(' ')
      const wordsPerSegment = 20
      const segmentDuration = 10 // 10 segundos por segmento
      
      for (let i = 0; i < words.length; i += wordsPerSegment) {
        const segmentWords = words.slice(i, i + wordsPerSegment)
        const segmentText = segmentWords.join(' ')
        
        segments.push({
          start: Math.floor((i / wordsPerSegment) * segmentDuration),
          duration: segmentDuration,
          text: segmentText,
          offset: Math.floor(i / wordsPerSegment)
        })
      }
      
      console.log(`‚úÖ ${segments.length} segmentos convertidos do Gemini`)
      return segments
      
    } catch (error) {
      console.error('‚ùå Erro na convers√£o do Gemini:', error)
      return []
    }
  }

  /**
   * Converte resultado do OpenAI Whisper para nosso formato
   */
  private static convertOpenAIResultToSegments(transcription: any): TranscriptSegment[] {
    try {
      console.log(`üîÑ Convertendo resultado do OpenAI Whisper...`)
      
      const segments: TranscriptSegment[] = []
      
      if (transcription.segments && Array.isArray(transcription.segments)) {
        // Formato verbose_json com segmentos
        transcription.segments.forEach((segment: any, index: number) => {
          if (segment.text && segment.start !== undefined && segment.end !== undefined) {
            segments.push({
              start: Math.floor(segment.start),
              duration: Math.floor(segment.end - segment.start),
              text: segment.text.trim(),
              offset: index
            })
          }
        })
      } else if (transcription.text) {
        // Formato simples, dividir em segmentos
        const text = transcription.text
        const words = text.split(' ')
        const wordsPerSegment = 20
        const segmentDuration = 10 // 10 segundos por segmento
        
        for (let i = 0; i < words.length; i += wordsPerSegment) {
          const segmentWords = words.slice(i, i + wordsPerSegment)
          const segmentText = segmentWords.join(' ')
          
          segments.push({
            start: Math.floor((i / wordsPerSegment) * segmentDuration),
            duration: segmentDuration,
            text: segmentText,
            offset: Math.floor(i / wordsPerSegment)
          })
        }
      }
      
      console.log(`‚úÖ ${segments.length} segmentos convertidos do OpenAI Whisper`)
      return segments
      
    } catch (error) {
      console.error('‚ùå Erro na convers√£o do OpenAI:', error)
      return []
    }
  }

  /**
   * Transcreve √°udio via Gemini (Google) - Fallback principal
   */
  static async transcribeWithGemini(audioFilePath: string): Promise<TranscriptionResult> {
    try {
      const apiKey = process.env.GEMINI_API_KEY
      if (!apiKey) {
        return {
          success: false,
          segments: [],
          error: 'Gemini API Key n√£o configurada'
        }
      }

      console.log(`üéµ Transcrevendo √°udio via Gemini: ${audioFilePath}`)
      const startTime = Date.now()

      // Inicializar Gemini
      const genAI = new GoogleGenerativeAI(apiKey)
      const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" })

      // Ler arquivo como base64
      const audioBuffer = fs.readFileSync(audioFilePath)
      const base64Audio = audioBuffer.toString('base64')
      
      console.log(`üìä Arquivo convertido para base64: ${base64Audio.length} caracteres`)

      // Prompt para transcri√ß√£o
      const prompt = `Transcreva este √°udio em portugu√™s brasileiro. Retorne apenas o texto transcrito, sem formata√ß√£o adicional. Se houver m√∫ltiplos falantes, identifique-os.`

      // Enviar para Gemini com √°udio
      const result = await model.generateContent([
        prompt,
        {
          inlineData: {
            mimeType: "audio/mpeg",
            data: base64Audio
          }
        }
      ])

      const response = await result.response
      const transcriptionText = response.text()

      const processingTime = Date.now() - startTime
      console.log(`‚úÖ Transcri√ß√£o Gemini conclu√≠da em ${processingTime}ms`)

      // Converter para segmentos
      const segments = this.convertGeminiResultToSegments(transcriptionText)
      
      return {
        success: true,
        segments,
        processingTime
      }

    } catch (error) {
      console.error('‚ùå Erro na transcri√ß√£o Gemini:', error)
      return {
        success: false,
        segments: [],
        error: (error as Error).message
      }
    }
  }

  /**
   * Transcreve √°udio via AssemblyAI (alternativa)
   */
  static async transcribeWithAssemblyAI(audioFilePath: string): Promise<TranscriptionResult> {
    try {
      const apiKey = process.env.ASSEMBLYAI_API_KEY
      if (!apiKey) {
        return {
          success: false,
          segments: [],
          error: 'AssemblyAI API Key n√£o configurada'
        }
      }

      console.log(`üéµ Transcrevendo √°udio via AssemblyAI: ${audioFilePath}`)
      const startTime = Date.now()

      // Implementar AssemblyAI
      console.log(`‚ö†Ô∏è AssemblyAI n√£o implementado ainda - arquivo: ${audioFilePath}`)
      
      return {
        success: false,
        segments: [],
        error: 'AssemblyAI n√£o implementado ainda'
      }

    } catch (error) {
      console.error('‚ùå Erro na transcri√ß√£o AssemblyAI:', error)
      return {
        success: false,
        segments: [],
        error: (error as Error).message
      }
    }
  }

  /**
   * Transcreve √°udio via m√∫ltiplas APIs (fallback autom√°tico)
   */
  static async transcribeWithMultipleAPIs(audioFilePath: string): Promise<TranscriptionResult> {
    console.log(`üîÑ Tentando transcri√ß√£o via m√∫ltiplas APIs...`)
    
    // Tentar OpenAI primeiro
    const openaiResult = await this.transcribeWithOpenAI(audioFilePath)
    if (openaiResult.success) {
      console.log(`‚úÖ OpenAI funcionou, retornando resultado`)
      return openaiResult
    }

    // Tentar Gemini como fallback principal
    console.log(`üîÑ OpenAI falhou, tentando Gemini...`)
    const geminiResult = await this.transcribeWithGemini(audioFilePath)
    if (geminiResult.success) {
      console.log(`‚úÖ Gemini funcionou, retornando resultado`)
      return geminiResult
    }

    // Tentar AssemblyAI como √∫ltimo fallback
    console.log(`üîÑ Gemini falhou, tentando AssemblyAI...`)
    const assemblyResult = await this.transcribeWithAssemblyAI(audioFilePath)
    if (assemblyResult.success) {
      console.log(`‚úÖ AssemblyAI funcionou, retornando resultado`)
      return assemblyResult
    }

    // Nenhuma API funcionou
    console.log(`‚ùå Nenhuma API de transcri√ß√£o funcionou`)
    return {
      success: false,
      segments: [],
      error: `Todas as APIs falharam. OpenAI: ${openaiResult.error}, Gemini: ${geminiResult.error}, AssemblyAI: ${assemblyResult.error}`
    }
  }

  /**
   * Processa arquivo de √°udio para transcri√ß√£o
   */
  static async processAudioFile(audioFilePath: string): Promise<TranscriptionResult> {
    try {
      console.log(`üéµ Processando arquivo de √°udio: ${audioFilePath}`)
      
      // Verificar se o arquivo existe
      if (!fs.existsSync(audioFilePath)) {
        return {
          success: false,
          segments: [],
          error: `Arquivo n√£o encontrado: ${audioFilePath}`
        }
      }
      
      // Fazer transcri√ß√£o
      const result = await this.transcribeWithMultipleAPIs(audioFilePath)
      
      if (result.success) {
        console.log(`üéâ Transcri√ß√£o conclu√≠da com sucesso: ${result.segments.length} segmentos`)
      } else {
        console.log(`‚ùå Falha na transcri√ß√£o: ${result.error}`)
      }
      
      return result

    } catch (error) {
      console.error('‚ùå Erro no processamento de √°udio:', error)
      return {
        success: false,
        segments: [],
        error: (error as Error).message
      }
    }
  }

  /**
   * Converte resultado de transcri√ß√£o para formato padr√£o
   */
  static convertToTranscriptSegments(rawTranscription: any): TranscriptSegment[] {
    try {
      console.log(`üîÑ Convertendo resultado de transcri√ß√£o...`)
      
      const segments: TranscriptSegment[] = []
      
      // Implementar convers√£o baseada no formato da API
      // Por enquanto, retornamos vazio
      
      console.log(`‚ö†Ô∏è Convers√£o de transcri√ß√£o n√£o implementada`)
      return segments

    } catch (error) {
      console.error('‚ùå Erro na convers√£o de transcri√ß√£o:', error)
      return []
    }
  }
}
